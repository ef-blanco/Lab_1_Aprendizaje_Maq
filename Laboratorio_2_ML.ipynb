{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e98db9",
   "metadata": {},
   "source": [
    "# Laboratorio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b7d6e",
   "metadata": {},
   "source": [
    "Integrantes del grupo:\n",
    "1. Emmanuel Blanco - 202312743\n",
    "2. Juan David Guzmán - 202320890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ae411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, RobustScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, validation_curve\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da5566",
   "metadata": {},
   "source": [
    "## Construcción del modelo de regresión polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4c6b2",
   "metadata": {},
   "source": [
    "### Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca67a69",
   "metadata": {},
   "source": [
    "Al igual que en el laboratorio anterior, el primer paso que realizamos en la preparación de datos es eliminar las filas de Id repetidos, y eliminar las filas con CVD Risk Score nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88870a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de quitar duplicados: 1376\n",
      "Después de quitar nulos en objetivo: 1348\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"./data/Datos_Lab_1.csv\")\n",
    "data = training_data.drop_duplicates(subset='Patient ID', keep='last')\n",
    "print(f\"Después de quitar duplicados: {data.shape[0]}\")\n",
    "\n",
    "data = data.dropna(subset=['CVD Risk Score'])\n",
    "print(f\"Después de quitar nulos en objetivo: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff84c6",
   "metadata": {},
   "source": [
    "Una vez hechos estos primeros cambios hacemos la división de los datos para entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac92580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension datos entrenamiento:\n",
      "Training x: (1011, 23)\n",
      "Training y: (1011,)\n",
      "\n",
      "Dimensión datos de prueba: \n",
      "Test x: (337, 23)\n",
      "Test y: (337,)\n"
     ]
    }
   ],
   "source": [
    "target = 'CVD Risk Score'\n",
    "x = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "print(\"Dimension datos entrenamiento:\")\n",
    "print(f\"Training x: {x_train.shape}\")\n",
    "print(f\"Training y: {y_train.shape}\")\n",
    "print(\"\\nDimensión datos de prueba: \")\n",
    "print(f\"Test x: {x_test.shape}\")\n",
    "print(f\"Test y: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db240a32",
   "metadata": {},
   "source": [
    "A partir de este punto construiremos el pipeline encargado de las demás transformaciones de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76bf1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guardan en una lista las columnas que no consideraremos para este modelo\n",
    "cols_to_drop = ['Patient ID', 'Date of Service', 'Blood Pressure (mmHg)','Blood Pressure Category', 'Height (cm)']\n",
    "\n",
    "#Identificamos las columnas numéricas y categóricas\n",
    "numeric_features = ['Age', 'Weight (kg)', 'Height (m)', 'BMI', 'Abdominal Circumference (cm)', 'Total Cholesterol (mg/dL)', \n",
    "                    'HDL (mg/dL)', 'Fasting Blood Sugar (mg/dL)','Waist-to-Height Ratio', 'Systolic BP', 'Diastolic BP', \n",
    "                    'Estimated LDL (mg/dL)']\n",
    "\n",
    "categorical_features = ['Sex', 'Smoking Status', 'Diabetes Status', 'Physical Activity Level', 'Family History of CVD']\n",
    "\n",
    "# Definimos la función que será usada para quitar las columnas que no necesitamos\n",
    "def drop_columns(df):\n",
    "    return df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "dropper = FunctionTransformer(drop_columns)\n",
    "\n",
    "# Definimos los transformadores para los dos tipos de datos\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\",RobustScaler()),\n",
    "    (\"polynomial\",PolynomialFeatures(degree=2)),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\")),\n",
    "])\n",
    "\n",
    "# Unimos los transformadores de datos con un ColumnTransformer\n",
    "\n",
    "preprocessor_polynomial = ColumnTransformer(transformers=[\n",
    "    (\"num\",numeric_transformer,numeric_features),\n",
    "    (\"cat\",categorical_transformer,categorical_features),\n",
    "])\n",
    "\n",
    "# definimos un función que se deshaga de los valores negativos\n",
    "def corregir_negativos(df):\n",
    "    df = df.copy()\n",
    "    for column in numeric_features:\n",
    "        df[column] = df[column].abs()\n",
    "    return df\n",
    "\n",
    "correcion_negs = FunctionTransformer(corregir_negativos)\n",
    "\n",
    "# Finalmente montamos el pipeline para la regresión polinomial\n",
    "pipeline_reg_polinom = Pipeline(steps=[\n",
    "    (\"dropper\",dropper),\n",
    "    (\"correccion_negativos\",correcion_negs),\n",
    "    (\"preprocesamiento\",preprocessor_polynomial),\n",
    "    (\"modelo\",LinearRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa3b8e",
   "metadata": {},
   "source": [
    "Con el pipeline definido, ahora usaremos GridSearchCV para encontrar los mejores hiperparámetros para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "193b88bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros obtenidos por GridSearch:\n",
      "\n",
      "Mejor estrategia de imputación para datos numéricos:  most_frequent\n",
      "Mejor grado para PolynomialFeautures:  2\n",
      "Mejor escalador de datos numéricos:  StandardScaler()\n",
      "Número de coeficientes del modelo:  98\n"
     ]
    }
   ],
   "source": [
    "#Definimos los parámetros que queremmos que el GridSearchCV pruebe\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocesamiento__num__scaler\":[StandardScaler(),RobustScaler(),MinMaxScaler()],\n",
    "    \"preprocesamiento__num__imputer__strategy\":[\"mean\",\"median\",\"most_frequent\"],\n",
    "    \"preprocesamiento__num__polynomial__degree\":[2,3,4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline_reg_polinom,param_grid=param_grid,cv=10,scoring=\"neg_root_mean_squared_error\",n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_\n",
    "\n",
    "print(\"Mejores parámetros obtenidos por GridSearch:\\n\")\n",
    "print(\"Mejor estrategia de imputación para datos numéricos: \",grid_search.best_params_[\"preprocesamiento__num__imputer__strategy\"])\n",
    "print(\"Mejor grado para PolynomialFeautures: \",grid_search.best_params_[\"preprocesamiento__num__polynomial__degree\"])\n",
    "print(\"Mejor escalador de datos numéricos: \",grid_search.best_params_[\"preprocesamiento__num__scaler\"])\n",
    "\n",
    "mejor_modelo = grid_search.best_estimator_.named_steps[\"modelo\"]\n",
    "print(\"Número de coeficientes del modelo: \",len(mejor_modelo.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f236a",
   "metadata": {},
   "source": [
    "Ahora hacemos las predicciones y calculamos la métricas necesarias para validar la regresión polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dea70d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Modelo de regresión polinomial grado 2 ----\n",
      "RMSE: 9.80\n",
      "MAE: 3.75\n",
      "R²: 0.17\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = grid_search.best_estimator_.predict(x_train)\n",
    "\n",
    "#Calculamos las métricas\n",
    "print(f'------ Modelo de regresión polinomial grado {grid_search.best_params_[\"preprocesamiento__num__polynomial__degree\"]} ----')\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred):.2f}\")\n",
    "print(f'R²: {r2_score(y_train, y_train_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5d209",
   "metadata": {},
   "source": [
    "## Generación de curvas de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f50b72e",
   "metadata": {},
   "source": [
    "Buscamos mostrar cómo cambia el error cuando se aumenta la complejidad del modelo (el grado del polinomio).Se entrena con ese proposito el mismo pipeline variando el grado de 1 a 4 con validación cruzada de 5 folds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db5b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pipeline limpio para curvas de validación\n",
    "preprocessor_curvas = ColumnTransformer(transformers=[\n",
    "    (\"num\", Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"polynomial\", PolynomialFeatures(include_bias=False)),\n",
    "    ]), numeric_features),\n",
    "    (\"cat\", Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\")),\n",
    "    ]), categorical_features),\n",
    "])\n",
    "\n",
    "pipeline_val_curve = Pipeline(steps=[\n",
    "    (\"dropper\", dropper),\n",
    "    (\"correccion_negativos\", correcion_negs),\n",
    "    (\"preprocesamiento\", preprocessor_curvas),\n",
    "    (\"modelo\", LinearRegression())\n",
    "])\n",
    "\n",
    "# Curva de validación: grado del polinomio\n",
    "param_range_grado = [1, 2, 3, 4]\n",
    "\n",
    "train_scores_vc, val_scores_vc = validation_curve(\n",
    "    pipeline_val_curve,\n",
    "    x_train, y_train,\n",
    "    param_name=\"preprocesamiento__num__polynomial__degree\",\n",
    "    param_range=param_range_grado,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "train_mean_vc = -train_scores_vc.mean(axis=1)\n",
    "train_std_vc  =  train_scores_vc.std(axis=1)\n",
    "val_mean_vc   = -val_scores_vc.mean(axis=1)\n",
    "val_std_vc    =  val_scores_vc.std(axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(param_range_grado, train_mean_vc, 'o-', color='steelblue', label='Error de entrenamiento (RMSE)')\n",
    "ax.fill_between(param_range_grado,\n",
    "                train_mean_vc - train_std_vc,\n",
    "                train_mean_vc + train_std_vc,\n",
    "                alpha=0.15, color='steelblue')\n",
    "ax.plot(param_range_grado, val_mean_vc, 'o-', color='tomato', label='Error de validación (RMSE)')\n",
    "ax.fill_between(param_range_grado,\n",
    "                val_mean_vc - val_std_vc,\n",
    "                val_mean_vc + val_std_vc,\n",
    "                alpha=0.15, color='tomato')\n",
    "ax.set_xlabel('Grado del polinomio', fontsize=12)\n",
    "ax.set_ylabel('RMSE', fontsize=12)\n",
    "ax.set_title('Curva de validación: Grado del polinomio vs RMSE', fontsize=14)\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nResumen curvas de validación:\")\n",
    "print(f\"{'Grado':<10} {'RMSE Train':<18} {'RMSE Val':<18}\")\n",
    "print(\"-\" * 46)\n",
    "for g, tr, va in zip(param_range_grado, train_mean_vc, val_mean_vc):\n",
    "    print(f\"{g:<10} {tr:<18.4f} {va:<18.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a12120e",
   "metadata": {},
   "source": [
    "Se produjo un grafico con dos lineas. Una roja que muestra el error de validación (baja, pero luego sube = sobreajuste) y una azul que señala el error de entrenamiento (siempre baja al subir el grado). Y pues la brecha entre las dos curvas te dice en qué grado empieza el sobreajuste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ca41f",
   "metadata": {},
   "source": [
    "## Construcción de modelos de regresión lineal regularizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f78e89",
   "metadata": {},
   "source": [
    "Pues son 2 modelos de reg. Lineal que tienen regularización (L2 y L1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11b126",
   "metadata": {},
   "source": [
    "1. L2 o Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_lineal_ridge = ColumnTransformer(transformers=[\n",
    "    (\"num\", Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]), numeric_features),\n",
    "    (\"cat\", Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\")),\n",
    "    ]), categorical_features),\n",
    "])\n",
    "\n",
    "pipeline_ridge = Pipeline(steps=[\n",
    "    (\"dropper\", dropper),\n",
    "    (\"correccion_negativos\", correcion_negs),\n",
    "    (\"preprocesamiento\", preprocessor_lineal_ridge),\n",
    "    (\"modelo\", Ridge())\n",
    "])\n",
    "\n",
    "param_grid_ridge = {\n",
    "    \"preprocesamiento__num__imputer__strategy\": [\"mean\", \"median\", \"most_frequent\"],\n",
    "    \"preprocesamiento__num__scaler\": [StandardScaler(), RobustScaler(), MinMaxScaler()],\n",
    "    \"modelo__alpha\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    pipeline_ridge, param_grid=param_grid_ridge, cv=10,\n",
    "    scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n",
    ")\n",
    "grid_search_ridge.fit(x_train, y_train)\n",
    "\n",
    "print(\"=== Mejores parámetros Ridge ===\")\n",
    "print(f\"  Estrategia imputación : {grid_search_ridge.best_params_['preprocesamiento__num__imputer__strategy']}\")\n",
    "print(f\"  Escalador             : {grid_search_ridge.best_params_['preprocesamiento__num__scaler']}\")\n",
    "print(f\"  Alpha                 : {grid_search_ridge.best_params_['modelo__alpha']}\")\n",
    "\n",
    "y_pred_ridge_train = grid_search_ridge.best_estimator_.predict(x_train)\n",
    "print(f\"\\n--- Métricas Ridge (entrenamiento) ---\")\n",
    "print(f\"RMSE : {np.sqrt(mean_squared_error(y_train, y_pred_ridge_train)):.4f}\")\n",
    "print(f\"MAE  : {mean_absolute_error(y_train, y_pred_ridge_train):.4f}\")\n",
    "print(f\"R²   : {r2_score(y_train, y_pred_ridge_train):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949ee0cd",
   "metadata": {},
   "source": [
    "Aquí penalizamos coeficientes grandes pero no los eliminamos. GridSearchCV en este caso busca el mejor alpha entre 7 valores, 3 escaladores y 3 estrategias de imputación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641a65ee",
   "metadata": {},
   "source": [
    "2. L1 o Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cacd4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_lineal_lasso = ColumnTransformer(transformers=[\n",
    "    (\"num\", Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "    ]), numeric_features),\n",
    "    (\"cat\", Pipeline(steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\")),\n",
    "    ]), categorical_features),\n",
    "])\n",
    "\n",
    "pipeline_lasso = Pipeline(steps=[\n",
    "    (\"dropper\", dropper),\n",
    "    (\"correccion_negativos\", correcion_negs),\n",
    "    (\"preprocesamiento\", preprocessor_lineal_lasso),\n",
    "    (\"modelo\", Lasso(max_iter=10000))\n",
    "])\n",
    "\n",
    "param_grid_lasso = {\n",
    "    \"preprocesamiento__num__imputer__strategy\": [\"mean\", \"median\", \"most_frequent\"],\n",
    "    \"preprocesamiento__num__scaler\": [StandardScaler(), RobustScaler(), MinMaxScaler()],\n",
    "    \"modelo__alpha\": [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "\n",
    "grid_search_lasso = GridSearchCV(\n",
    "    pipeline_lasso, param_grid=param_grid_lasso, cv=10,\n",
    "    scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n",
    ")\n",
    "grid_search_lasso.fit(x_train, y_train)\n",
    "\n",
    "print(\"\\n=== Mejores parámetros Lasso ===\")\n",
    "print(f\"  Estrategia imputación : {grid_search_lasso.best_params_['preprocesamiento__num__imputer__strategy']}\")\n",
    "print(f\"  Escalador             : {grid_search_lasso.best_params_['preprocesamiento__num__scaler']}\")\n",
    "print(f\"  Alpha                 : {grid_search_lasso.best_params_['modelo__alpha']}\")\n",
    "\n",
    "y_pred_lasso_train = grid_search_lasso.best_estimator_.predict(x_train)\n",
    "print(f\"\\n--- Métricas Lasso (entrenamiento) ---\")\n",
    "print(f\"RMSE : {np.sqrt(mean_squared_error(y_train, y_pred_lasso_train)):.4f}\")\n",
    "print(f\"MAE  : {mean_absolute_error(y_train, y_pred_lasso_train):.4f}\")\n",
    "print(f\"R²   : {r2_score(y_train, y_pred_lasso_train):.4f}\")\n",
    "\n",
    "# Variables seleccionadas por Lasso (coeficientes distintos de cero)\n",
    "modelo_lasso_fitted = grid_search_lasso.best_estimator_.named_steps[\"modelo\"]\n",
    "preprocessor_lasso_fitted = grid_search_lasso.best_estimator_.named_steps[\"preprocesamiento\"]\n",
    "\n",
    "feature_names_cat = (preprocessor_lasso_fitted\n",
    "                     .transformers_[1][1]\n",
    "                     .named_steps[\"onehot\"]\n",
    "                     .get_feature_names_out(categorical_features)\n",
    "                     .tolist())\n",
    "all_feature_names = numeric_features + feature_names_cat\n",
    "coef_lasso_df = pd.DataFrame({\"Feature\": all_feature_names,\n",
    "                               \"Coeficiente\": modelo_lasso_fitted.coef_})\n",
    "coef_nonzero = (coef_lasso_df[coef_lasso_df[\"Coeficiente\"] != 0]\n",
    "                .sort_values(\"Coeficiente\", ascending=False))\n",
    "print(f\"\\nVariables seleccionadas por Lasso (coef ≠ 0): \"\n",
    "      f\"{len(coef_nonzero)} de {len(all_feature_names)}\")\n",
    "print(coef_nonzero.to_string(index=False))\n",
    "\n",
    "# Gráfico de coeficientes Lasso\n",
    "if len(coef_nonzero) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, max(4, len(coef_nonzero) * 0.4)))\n",
    "    colors = ['steelblue' if c > 0 else 'tomato' for c in coef_nonzero[\"Coeficiente\"]]\n",
    "    ax.barh(coef_nonzero[\"Feature\"], coef_nonzero[\"Coeficiente\"], color=colors, alpha=0.8)\n",
    "    ax.axvline(0, color='black', linewidth=0.8)\n",
    "    ax.set_title(\"Coeficientes del modelo Lasso (variables seleccionadas)\", fontsize=13)\n",
    "    ax.set_xlabel(\"Coeficiente\")\n",
    "    ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5318a46f",
   "metadata": {},
   "source": [
    "Hacemos lo mismo que L2 pero además fOrzaMOS algunos coeficientes a exactamente cero, es decir, L1 selecciona variables automáticamente. Al final esto imprime qué variables quedaron y muestra un gráfico de barras con sus coeficientes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa24df1",
   "metadata": {},
   "source": [
    "## Construcción de un modelo de regresión polinomial regularizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7ecbc",
   "metadata": {},
   "source": [
    "## Comparación y selección del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa445df",
   "metadata": {},
   "source": [
    "## Construcción de intervalos de confianza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb0afe",
   "metadata": {},
   "source": [
    "## Análisis de resultados\n",
    "\n",
    "### Análisis cuantitativo.\n",
    "\n",
    "- ¿Cuál modelo obtuvo el mejor desempeño en el conjunto de test?\n",
    "\n",
    "- ¿Coincide el mejor desempeño en test con el mejor promedio en validación cruzada? Si no coincide, ¿cuál puede ser la explicación?\n",
    "\n",
    "- ¿El modelo con mejor métrica promedio es necesariamente el más adecuado? Justifica considerando también la desviación estándar del desempeño.\n",
    "\n",
    "- Con base en las curvas de validación, ¿cómo cambia el error a medida que aumenta la complejidad? ¿En qué punto se evidencia sobreajuste?\n",
    "\n",
    "- ¿Cómo afecta la regularización la magnitud y estabilidad de los coeficientes?\n",
    "\n",
    "- ¿Los intervalos de confianza obtenidos mediante bootstrapping sugieren estabilidad o alta variabilidad en el desempeño? ¿Qué implicaciones tiene esto?\n",
    "\n",
    "### Análisis cualitativo.\n",
    "\n",
    "- ¿Qué variables fueron seleccionadas como más relevantes por el modelo Lasso?\n",
    "\n",
    "- ¿Qué interpretación práctica tienen los coeficientes del modelo final en el contexto del riesgo cardiovascular?\n",
    "\n",
    "- ¿Existen diferencias relevantes entre el modelo más preciso y el más interpretable?\n",
    "\n",
    "- ¿Qué decisiones estratégicas podría tomar AlpesHearth a partir de los resultados obtenidos?\n",
    "\n",
    "- ¿Mayor precisión implica necesariamente mayor valor para la organización?\n",
    "\n",
    "- ¿Un modelo más complejo necesariamente genera mayor valor empresarial? Discute considerando interpretabilidad, estabilidad y costo de implementación.\n",
    "\n",
    "### Reflexión conceptual.\n",
    "\n",
    "- ¿Qué relación observas entre complejidad del modelo, capacidad de generalización y estabilidad del desempeño?\n",
    "\n",
    "- ¿Qué fuentes de sesgo podrían estar presentes en los datos o en el proceso de modelado?\n",
    "\n",
    "- Si el tamaño de muestra fuera mayor, ¿esperarías cambios en la estabilidad de los modelos? Explique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
