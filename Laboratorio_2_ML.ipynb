{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e98db9",
   "metadata": {},
   "source": [
    "# Laboratorio 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b7d6e",
   "metadata": {},
   "source": [
    "Integrantes del grupo:\n",
    "1. Emmanuel Blanco - 202312743\n",
    "2. Juan David Guzmán - 202320890"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20ae411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, RobustScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, validation_curve\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da5566",
   "metadata": {},
   "source": [
    "## Construcción del modelo de regresión polinomial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4c6b2",
   "metadata": {},
   "source": [
    "### Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca67a69",
   "metadata": {},
   "source": [
    "Al igual que en el laboratorio anterior, el primer paso que realizamos en la preparación de datos es eliminar las filas de Id repetidos, y eliminar las filas con CVD Risk Score nulo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88870a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Después de quitar duplicados: 1376\n",
      "Después de quitar nulos en objetivo: 1348\n"
     ]
    }
   ],
   "source": [
    "training_data = pd.read_csv(\"./data/Datos_Lab_1.csv\")\n",
    "data = training_data.drop_duplicates(subset='Patient ID', keep='last')\n",
    "print(f\"Después de quitar duplicados: {data.shape[0]}\")\n",
    "\n",
    "data = data.dropna(subset=['CVD Risk Score'])\n",
    "print(f\"Después de quitar nulos en objetivo: {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ff84c6",
   "metadata": {},
   "source": [
    "Una vez hechos estos primeros cambios hacemos la división de los datos para entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ac92580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension datos entrenamiento:\n",
      "Training x: (1011, 23)\n",
      "Training y: (1011,)\n",
      "\n",
      "Dimensión datos de prueba: \n",
      "Test x: (337, 23)\n",
      "Test y: (337,)\n"
     ]
    }
   ],
   "source": [
    "target = 'CVD Risk Score'\n",
    "x = data.drop(columns=[target])\n",
    "y = data[target]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)\n",
    "print(\"Dimension datos entrenamiento:\")\n",
    "print(f\"Training x: {x_train.shape}\")\n",
    "print(f\"Training y: {y_train.shape}\")\n",
    "print(\"\\nDimensión datos de prueba: \")\n",
    "print(f\"Test x: {x_test.shape}\")\n",
    "print(f\"Test y: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db240a32",
   "metadata": {},
   "source": [
    "A partir de este punto construiremos el pipeline encargado de las demás transformaciones de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76bf1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se guardan en una lista las columnas que no consideraremos para este modelo\n",
    "cols_to_drop = ['Patient ID', 'Date of Service', 'Blood Pressure (mmHg)','Blood Pressure Category', 'Height (cm)']\n",
    "\n",
    "#Identificamos las columnas numéricas y categóricas\n",
    "numeric_features = ['Age', 'Weight (kg)', 'Height (m)', 'BMI', 'Abdominal Circumference (cm)', 'Total Cholesterol (mg/dL)', \n",
    "                    'HDL (mg/dL)', 'Fasting Blood Sugar (mg/dL)','Waist-to-Height Ratio', 'Systolic BP', 'Diastolic BP', \n",
    "                    'Estimated LDL (mg/dL)']\n",
    "\n",
    "categorical_features = ['Sex', 'Smoking Status', 'Diabetes Status', 'Physical Activity Level', 'Family History of CVD']\n",
    "\n",
    "# Definimos la función que será usada para quitar las columnas que no necesitamos\n",
    "def drop_columns(df):\n",
    "    return df.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "dropper = FunctionTransformer(drop_columns)\n",
    "\n",
    "# Definimos los transformadores para los dos tipos de datos\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\",RobustScaler()),\n",
    "    (\"polynomial\",PolynomialFeatures(degree=2)),\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\",SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\",OneHotEncoder(handle_unknown=\"ignore\", drop=\"if_binary\")),\n",
    "])\n",
    "\n",
    "# Unimos los transformadores de datos con un ColumnTransformer\n",
    "\n",
    "preprocessor_polynomial = ColumnTransformer(transformers=[\n",
    "    (\"num\",numeric_transformer,numeric_features),\n",
    "    (\"cat\",categorical_transformer,categorical_features),\n",
    "])\n",
    "\n",
    "# definimos un función que se deshaga de los valores negativos\n",
    "def corregir_negativos(df):\n",
    "    df = df.copy()\n",
    "    for column in numeric_features:\n",
    "        df[column] = df[column].abs()\n",
    "    return df\n",
    "\n",
    "correcion_negs = FunctionTransformer(corregir_negativos)\n",
    "\n",
    "# Finalmente montamos el pipeline para la regresión polinomial\n",
    "pipeline_reg_polinom = Pipeline(steps=[\n",
    "    (\"dropper\",dropper),\n",
    "    (\"correccion_negativos\",correcion_negs),\n",
    "    (\"preprocesamiento\",preprocessor_polynomial),\n",
    "    (\"modelo\",LinearRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa3b8e",
   "metadata": {},
   "source": [
    "Con el pipeline definido, ahora usaremos GridSearchCV para encontrar los mejores hiperparámetros para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "193b88bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros obtenidos por GridSearch:\n",
      "\n",
      "Mejor estrategia de imputación para datos numéricos:  most_frequent\n",
      "Mejor grado para PolynomialFeautures:  2\n",
      "Mejor escalador de datos numéricos:  StandardScaler()\n",
      "Número de coeficientes del modelo:  98\n"
     ]
    }
   ],
   "source": [
    "#Definimos los parámetros que queremmos que el GridSearchCV pruebe\n",
    "\n",
    "param_grid = {\n",
    "    \"preprocesamiento__num__scaler\":[StandardScaler(),RobustScaler(),MinMaxScaler()],\n",
    "    \"preprocesamiento__num__imputer__strategy\":[\"mean\",\"median\",\"most_frequent\"],\n",
    "    \"preprocesamiento__num__polynomial__degree\":[2,3,4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline_reg_polinom,param_grid=param_grid,cv=10,scoring=\"neg_root_mean_squared_error\",n_jobs=-1)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_search.best_params_\n",
    "\n",
    "print(\"Mejores parámetros obtenidos por GridSearch:\\n\")\n",
    "print(\"Mejor estrategia de imputación para datos numéricos: \",grid_search.best_params_[\"preprocesamiento__num__imputer__strategy\"])\n",
    "print(\"Mejor grado para PolynomialFeautures: \",grid_search.best_params_[\"preprocesamiento__num__polynomial__degree\"])\n",
    "print(\"Mejor escalador de datos numéricos: \",grid_search.best_params_[\"preprocesamiento__num__scaler\"])\n",
    "\n",
    "mejor_modelo = grid_search.best_estimator_.named_steps[\"modelo\"]\n",
    "print(\"Número de coeficientes del modelo: \",len(mejor_modelo.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996f236a",
   "metadata": {},
   "source": [
    "Ahora hacemos las predicciones y calculamos la métricas necesarias para validar la regresión polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dea70d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Modelo de regresión polinomial grado 2 ----\n",
      "RMSE: 9.80\n",
      "MAE: 3.75\n",
      "R²: 0.17\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = grid_search.best_estimator_.predict(x_train)\n",
    "\n",
    "#Calculamos las métricas\n",
    "print(f'------ Modelo de regresión polinomial grado {grid_search.best_params_[\"preprocesamiento__num__polynomial__degree\"]} ----')\n",
    "print(f\"RMSE: {np.sqrt(mean_squared_error(y_train, y_train_pred)):.2f}\")\n",
    "print(f\"MAE: {mean_absolute_error(y_train, y_train_pred):.2f}\")\n",
    "print(f'R²: {r2_score(y_train, y_train_pred):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e5d209",
   "metadata": {},
   "source": [
    "## Generación de curvas de validación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ca41f",
   "metadata": {},
   "source": [
    "## Construcción de modelos de regresión lineal regularizados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa24df1",
   "metadata": {},
   "source": [
    "## Construcción de un modelo de regresión polinomial regularizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7ecbc",
   "metadata": {},
   "source": [
    "## Comparación y selección del mejor modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa445df",
   "metadata": {},
   "source": [
    "## Construcción de intervalos de confianza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedb0afe",
   "metadata": {},
   "source": [
    "## Análisis de resultados\n",
    "\n",
    "### Análisis cuantitativo.\n",
    "\n",
    "- ¿Cuál modelo obtuvo el mejor desempeño en el conjunto de test?\n",
    "\n",
    "- ¿Coincide el mejor desempeño en test con el mejor promedio en validación cruzada? Si no coincide, ¿cuál puede ser la explicación?\n",
    "\n",
    "- ¿El modelo con mejor métrica promedio es necesariamente el más adecuado? Justifica considerando también la desviación estándar del desempeño.\n",
    "\n",
    "- Con base en las curvas de validación, ¿cómo cambia el error a medida que aumenta la complejidad? ¿En qué punto se evidencia sobreajuste?\n",
    "\n",
    "- ¿Cómo afecta la regularización la magnitud y estabilidad de los coeficientes?\n",
    "\n",
    "- ¿Los intervalos de confianza obtenidos mediante bootstrapping sugieren estabilidad o alta variabilidad en el desempeño? ¿Qué implicaciones tiene esto?\n",
    "\n",
    "### Análisis cualitativo.\n",
    "\n",
    "- ¿Qué variables fueron seleccionadas como más relevantes por el modelo Lasso?\n",
    "\n",
    "- ¿Qué interpretación práctica tienen los coeficientes del modelo final en el contexto del riesgo cardiovascular?\n",
    "\n",
    "- ¿Existen diferencias relevantes entre el modelo más preciso y el más interpretable?\n",
    "\n",
    "- ¿Qué decisiones estratégicas podría tomar AlpesHearth a partir de los resultados obtenidos?\n",
    "\n",
    "- ¿Mayor precisión implica necesariamente mayor valor para la organización?\n",
    "\n",
    "- ¿Un modelo más complejo necesariamente genera mayor valor empresarial? Discute considerando interpretabilidad, estabilidad y costo de implementación.\n",
    "\n",
    "### Reflexión conceptual.\n",
    "\n",
    "- ¿Qué relación observas entre complejidad del modelo, capacidad de generalización y estabilidad del desempeño?\n",
    "\n",
    "- ¿Qué fuentes de sesgo podrían estar presentes en los datos o en el proceso de modelado?\n",
    "\n",
    "- Si el tamaño de muestra fuera mayor, ¿esperarías cambios en la estabilidad de los modelos? Explique."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
